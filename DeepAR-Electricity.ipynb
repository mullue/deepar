{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker/DeepAR demo on electricity dataset\n",
    "\n",
    "본 노트북은 다음 예제를 한글로 번역하고 일부 오류를 수정 반영하였습니다. [DeepAR electricity notebook](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity).\n",
    "<br>본 노트북은 다음 예제를 보완합니다. [DeepAR introduction notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb). \n",
    "\n",
    "본 예제에서 370명의 고객에 대한 시간별 에너지 사용량을 예측하는 유즈케이스를 SageMaker의 DeepAR로 어떻게 해결하는지 살펴볼 것입니다.  \n",
    "사용할 데이터셋에 대한 자세한 내용은 다음 링크와 [dataset](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014) 다음 논문들을 참고합니다. [[1](https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips29/reviews/526.html)] and [[2](https://arxiv.org/abs/1704.04110)].  \n",
    "\n",
    "본 예제를 통해 살펴볼 내용은 다음과 같습니다.\n",
    "* 데이터셋 준비하기\n",
    "* SageMaker Python SDK를 이용하여 DeepAR 모델을 학습하고 배포하기 \n",
    "* 배포된 모델에 예측을 위한 요청 실행하기 \n",
    "* DeepAR의 고급기능 살펴보기 : 결측치 처리, time feature 추가, 비정기 주기(frequency)와 카테고리 정보 사용하기\n",
    "\n",
    "학습시간은 ml.c4.xlarge에서 실행시 약 40분 정도가 걸립니다. 추론은 ml.m4.xlarge에서 실행합니다. (이후 endpoint를 더이상 사용하지 않는 경우 endpoint를 삭제하십시오.)\n",
    "\n",
    "보다 자세한 내용은 다음을 참고하십시오. [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) 또는 [paper](https://arxiv.org/abs/1704.04110), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요시 다음 값을 업데이트하십시오. (옵션)\n",
    "- S3 버킷과 prefix 이름 - 학습데이터와 모델데이터가 저장됩니다. S3의 리전은 본 노트북을 실행하는 리전과 동일해야 합니다. \n",
    "- IAM role arn - 학습과 호스팅에서 사용할 IAM 역할 (열할생성은 AWS IAM문서를 참고하십시오.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'deepar-electricity-demo-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습을 실행할 컨테이너를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에너지사용 데이터 import 및 s3에 업로드하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCI 데이터셋 리포지로부터 원본 데이터셋을 다운로드합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOST = \"https://archive.ics.uci.edu\"\n",
    "DATA_PATH = \"/ml/machine-learning-databases/00321/\"\n",
    "ARCHIVE_NAME = \"LD2011_2014.txt.zip\"\n",
    "FILE_NAME = ARCHIVE_NAME[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_report_hook(count, block_size, total_size):\n",
    "    mb = int(count * block_size // 1e6)\n",
    "    if count % 500 == 0:\n",
    "        sys.stdout.write(\"\\r{} MB downloaded\".format(mb))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "if not os.path.isfile(FILE_NAME):\n",
    "    print(\"downloading dataset (258MB), can take a few minutes depending on your connection\")\n",
    "    urlretrieve(DATA_HOST + DATA_PATH + ARCHIVE_NAME, ARCHIVE_NAME, reporthook=progress_report_hook)\n",
    "\n",
    "    print(\"\\nextracting data archive\")\n",
    "    zip_ref = zipfile.ZipFile(ARCHIVE_NAME, 'r')\n",
    "    zip_ref.extractall(\"./\")\n",
    "    zip_ref.close()\n",
    "else:\n",
    "    print(\"File found skipping download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 데이터셋을 Pandas time series로 로드하고 변환을 진행하겠습니다. Pandas 를 이용할 경우 index조정이나 리샘플링 등이 보다 용이합니다.   \n",
    "원본데이터는 15분 간격으로 에너지사용이 기록되어 있지만 여기서는 2시간 간격으로 조정하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(FILE_NAME, sep=\";\", index_col=0, parse_dates=True, decimal=',')\n",
    "num_timeseries = data.shape[1]\n",
    "data_kw = data.resample('2H').sum() / 8\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 10명의 사용자에 대하여 2014년부터 2주간의 에너지 사용량 time series를 그래프로 그려보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 10):\n",
    "    timeseries[i].loc[\"2014-01-01\":\"2014-01-14\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")    \n",
    "    axx[i].set_ylabel(\"kW consumption\")   \n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습, 검증데이터셋 분리\n",
    "\n",
    "종종 모델을 평가하고 하이퍼파라미터 튜닝을 실행하기 위해 별도의 데이터셋을 이용한 에러 평가 매트릭이 필요합니다. 여기서는 가용한 데이터셋을 학습과 테스트용 셋으로 나누겠습니다. 분류나 회귀와 같은 일반적인 머신러닝 작업에서는 주로 랜덤하게 샘플을 추출하여 학습과 테스트셋을 생성하지만, 시계열 예측과 같은 문제에서는 시간순서를 기준으로 학습과 테스트셋을 나누어야 합니다. \n",
    "\n",
    "본 예제에서는 시계열의 마지막 부분을 평가용도로 분리하고 시계열의 앞부분을 학습용으로 사용하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = datetime.timedelta(hours=2)\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014-01-01 부터 2014-09-01 까지의 데이터를 학습에 사용할 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2014-01-01 00:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2014-09-01 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR 알고리즘에서는 JSON 입력포맷을 요구합니다. JSON 오브젝트 형태로 time series를 표현해야 합니다. 가장 간단한 방법은 각 시계열을 ``start``에 해당하는 시계열 시작점과 ``target``에 해당하는 시계열값의 리스트형태로 구성하는 것입니다. \n",
    "\n",
    "보다 복잡한 케이스로 ``dynamic_feat`` 항목을 이용하여 다른 시계열값을 feature로 입력받을수 있고, ``cat`` 항목을 이용하여 명목형 feature를 입력받을 수 있습니다. 이부분은 본 예제의 후반부에서 다루어집니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training - 1*freq].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습용 데이터의 이후 기간 데이터를 테스트용 데이터로 사용합니다. 이 데이터는 학습셋으로부터 이후 7일의 예측을 실행한 후 예측값과 실제값을 비교하여 테스트 스코어를 계산하는데 사용됩니다.  \n",
    "1주일 이상 기간에 대한 모델의 성능을 측정하기 위해 우리는 테스트 데이터셋을 학습데이터 범위 이후 1, 2, 3, 4주까지 확장하겠습니다. 이런식으로 우리는 모델에 대한 *rolling evaluation*을 실행하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training + k * prediction_length * freq].tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 dictionary를 `jsonlines`형태로 저장합니다. (DeepAR에서는 zip으로 압축된 jsonlines와 parquet포맷을 지원합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 로컬환경에 데이터파일을 가지고 있습니다. 이 데이터셋을 S3로 복사합니다. 네트워크환경에 따라 이 부분은 수분 정도가 소요됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3에 복사한 데이터를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 학습을 위한 데이터셋 작업을 마무리하였습니다. 다음은 DeepAR을 이용하여 학습과 모델 생성 및 예측을 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습\n",
    "\n",
    "이제 학습작업을 실행하기 위한 Estimator를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 학습을 위한 하이퍼파라미터를 정의합니다. 예를 들어, time series의 주기(frequency)가 지정되었고, 과거시점을 바라보는 데이터포인트 범위가 context_length를 통해 지정되었고, 예측을 실행할 기간이 prediction_length를 이용하여 지정되었습니다. 모델과 관련하여 레이어의 수와 레이어당 셀의 수, likelihood function 등을 지정할 수 있으며 epoch수 batch size, learning rate 등과 같은 학습옵션을 지정할 수 있습니다.  \n",
    "본 예제에서는 기본값을 사용하겠습니다. 하이퍼파리미터를 보다 세밀하게 튜닝하려면 [Sagemaker Automated Model Tuning](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/)을 이용할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": '2H',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습을 실행할 준비가 되었습니다. SAgeMaker는 EC2 인스턴스를 시작하고 S3를 다운로드하고, 학습된 진행시킨 후, 학습된 모델을 저장할 것입니다. \n",
    "\n",
    "만약 본 예제처럼 `test`데이터 채널을 입력한 경우, DeepAR 은 이 테스트셋을 이용하여 모델의 정확도(accuracy) 매트릭을 계산할 것입니다. 계산은 테스트셋의 각 timeseries의 마지막 `prediction_lengh`포인트를 예측하고 이를 실제값과 비교하는 방식으로 이루어집니다. \n",
    "\n",
    "**Note:** 다음 셀의 실행은 10분 이상 걸릴 수 있습니다. 실행시간은 데이터사이즈, 모델복잡도, 학습옵션 등에 따라 달라집니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 예제에서는 테스트셋을 지정하였기 때문에 이를 이용한 평가 매트릭이 계산되고 로깅되었습니다. (로그의 마지막을 보십시오.)  \n",
    "매트릭에 대한 자세한 정보는 [our documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)를 참고하십시오. 이 매트릭을 이용하여 [Automated Model Tuning service](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/)를 통해 파라미터를 최적화하고 모델을 튜닝할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint와 predictor 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델이 학습되었고, 이를 이용하여 예측을 수행할 수 있도록 엔드포인트(Endpoint)에 배포하겠습니다. \n",
    "\n",
    "**Note: 본 예제를 완료한 후 반드시 endpoint를 삭제하십시오. 삭제코드는 본 노트북의 맨 아래에 있습니다. 반드시 이 부분을 실행하십시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endpoint에 질의를 던지고 예측결과를 받기 위해, 다음 유틸리티 클래스를 정의합니다. 이 클래스는 JSON strings가 아닌 `pandas.Series`를 요청으로 사용할 수 있도록 합니다. (SDK 참조 : https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html?highlight=realtimepredictor#sagemaker.predictor.RealTimePredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + datetime.timedelta(hours=2)\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "#         prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)       \n",
    "#         print(prediction_time)\n",
    "#         print(type(prediction_time))\n",
    "#         print(prediction_length)\n",
    "#         print(type(prediction_length))\n",
    "#         print(freq)\n",
    "#         print(type(freq))\n",
    "        \n",
    "        prediction_index = pd.date_range(prediction_time, prediction_time + freq * (prediction_length-1), freq=freq)\n",
    "#         print(prediction_index)\n",
    "        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 정의한 DeepARPredictor 클래스를 활용하여 모델을 배포하고 endpoint를 생성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 및 결과그래프 작성 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 예측을 위해 `predictor`를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts=timeseries[120], quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 모델이 예측한 결과를 그래프로 표현하는 함수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방금 정의한 함수를 이용하여 임의의 고객에 대한 미래 특정시점의 예측값을 볼 수 있습니다. \n",
    "\n",
    "각 요청에 대한 예측값은 실시간으로 모델을 호출하여 얻습니다.\n",
    "\n",
    "여기서 우리는 주말의 사무실의 전력소비를 예측합니다. \n",
    "보고자하는 시계열과 예측날짜를 선택하고 `Run Interact`를 클릭하여 엔드포인트를 호출한 결과로 리턴된 예측값을 그래프로 살펴봅니다. \n",
    "\n",
    "We can interact with the function previously defined, to look at the forecast of any customer at any point in (future) time. \n",
    "\n",
    "For each request, the predictions are obtained by calling our served model on the fly.\n",
    "\n",
    "Here we forecast the consumption of an office after week-end (note the lower week-end consumption). \n",
    "You can select any time series and any forecast date, just click on `Run Interact` to generate the predictions from our served endpoint and see the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(\n",
    "    customer_id=IntSlider(min=0, max=369, value=91, style=style), \n",
    "    forecast_day=IntSlider(min=0, max=100, value=51, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=1, max=20, value=1, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(customer_id, forecast_day, confidence, history_weeks_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=timeseries[customer_id],\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot * 12 * 7,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고급 기능\n",
    "\n",
    "지금까지 간단한 예제를 통해 DeepAR을 위한 데이터셋을 준비하고 실행하는 방법을 살펴보았습니다. \n",
    "\n",
    "DeepAR에는 다음 기능들이 추가로 제공됩니다:\n",
    "\n",
    "- 결측치 처리: DeepAR은 학습과 추론단계에서 모두 time series내의 결측치를 처리할 수 있습니다. \n",
    "- 추가 시계열 features: DeepAR은 hour of day 등 디폴트 time series를 추가로 제공합니다. 그리고 `dynamic_feat`를 이용하여 사용자 정의 시계열 feature도 입력할 수 있습니다. \n",
    "- 주기(frequency) 일반화 : 기본 주기(minutes `min`, hours `H`, days `D`, weeks `W`, month `M`)로부터 정수를 곱한 값을 지원합니다. (`15min`, `2H` 등)\n",
    "- 명목(category) 변수: 만약 timeseries가 다른 그룹들(제품그룹, 타입, 지역, 등)에 속해 있다면 이 정보는 `cat`항목을 이용하여 추가로 명목형 변수로 입력될 수 있습니다.\n",
    "\n",
    "바로 다음 예시를 통해 결측치 처리 기능을 확인하겠습니다. 이 부분은 앞서 진행한 에너지소비 데이터를 재사용하지만 기능의 소개를 위해 일부 인위적인 조작을 할 것입니다.\n",
    "- 결측치 지원기능 확인을 위해 임의로 time series의 일부를 마스킹합니다. \n",
    "- '특별일'을 위한 또 다른 timeseries를 생성하고 이 날은 관측값이 높게 나오도록 조작합니다. \n",
    "- '특별일'을 나타태는 timeseries를 학습의 feature로 입력합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 언급한 바와 같이 '특별일'을 생성하고 해당날짜에 timeseries값을 높에 나오도록 조정해 보겠습니다. 이 시뮬레이션은 실제 상황에서는 특정 기간에 실행되면서 여러분의 timeseries에 영향을 미치는 프로모션이나 이벤트가 될 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_special_day_feature(ts, fraction=0.05):\n",
    "    # First select random day indices (plus the forecast day)\n",
    "    num_days = (ts.index[-1] - ts.index[0]).days\n",
    "    rand_indices = list(np.random.randint(0, num_days, int(num_days * 0.1))) + [num_days]\n",
    "    \n",
    "    feature_value = np.zeros_like(ts)\n",
    "    for i in rand_indices:\n",
    "        feature_value[i * 12: (i + 1) * 12] = 1.0\n",
    "    feature = pd.Series(index=ts.index, data=feature_value)\n",
    "    return feature\n",
    "\n",
    "def drop_at_random(ts, drop_probability=0.1):\n",
    "    assert(0 <= drop_probability < 1)\n",
    "    random_mask = np.random.random(len(ts)) < drop_probability\n",
    "    return ts.mask(random_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_day_features = [create_special_day_feature(ts) for ts in timeseries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 10):\n",
    "    ax = axx[i]\n",
    "    ts = time_series_processed[i][:400]\n",
    "    ts.plot(ax=ax)\n",
    "    ax.set_ylim(-0.1 * ts.max(), ts.max())\n",
    "    ax2 = ax.twinx()\n",
    "    special_day_features[i][:400].plot(ax=ax2, color='g')\n",
    "    ax2.set_ylim(-0.2, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'특별일'의 생성 다음에는 임의로 특정 시점의 값을 제거해 보겠습니다.  \n",
    "아래 그림은 일부 샘플 timeseries를 보여주고 있습니다. 그림에서 초록색 그래프가 '특별일' 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_uplift = [ts * (1.0 + feat) for ts, feat in zip(timeseries, special_day_features)]\n",
    "time_series_processed = [drop_at_random(ts) for ts in timeseries_uplift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_data_new_features = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": encode_target(ts[start_dataset:end_training]),\n",
    "        \"dynamic_feat\": [special_day_features[i][start_dataset:end_training].tolist()]\n",
    "    }\n",
    "    for i, ts in enumerate(time_series_processed)\n",
    "]\n",
    "print(len(training_data_new_features))\n",
    "\n",
    "# as in our previous example, we do a rolling evaluation over the next 7 days\n",
    "num_test_windows = 7\n",
    "\n",
    "test_data_new_features = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": encode_target(ts[start_dataset:end_training + freq*k*prediction_length]),\n",
    "        \"dynamic_feat\": [special_day_features[i][start_dataset:end_training + freq*k*prediction_length].tolist()]\n",
    "    }\n",
    "#     freq = datetime.timedelta(hours=2)\n",
    "    \n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for i, ts in enumerate(timeseries_uplift)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_consistency(train_dataset, test_dataset=None):\n",
    "    d = train_dataset[0]\n",
    "    has_dynamic_feat = 'dynamic_feat' in d\n",
    "    if has_dynamic_feat:\n",
    "        num_dynamic_feat = len(d['dynamic_feat'])\n",
    "    has_cat = 'cat' in d\n",
    "    if has_cat:\n",
    "        num_cat = len(d['cat'])\n",
    "    \n",
    "    def check_ds(ds):\n",
    "        for i, d in enumerate(ds):\n",
    "            if has_dynamic_feat:\n",
    "                assert 'dynamic_feat' in d\n",
    "                assert num_dynamic_feat == len(d['dynamic_feat'])\n",
    "                for f in d['dynamic_feat']:\n",
    "                    assert len(d['target']) == len(f)\n",
    "            if has_cat:\n",
    "                assert 'cat' in d\n",
    "                assert len(d['cat']) == num_cat\n",
    "    check_ds(train_dataset)\n",
    "    if test_dataset is not None:\n",
    "        check_ds(test_dataset)\n",
    "        \n",
    "check_dataset_consistency(training_data_new_features, test_data_new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train_new_features.json\", training_data_new_features)\n",
    "write_dicts_to_file(\"test_new_features.json\", test_data_new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_data_path_new_features = \"s3://{}/{}-new-features/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path_new_features = \"s3://{}/{}-new-features/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "print('Uploading to S3 this may take a few minutes depending on your connection.')\n",
    "copy_to_s3(\"train_new_features.json\", s3_data_path_new_features + \"/train/train_new_features.json\", override=True)\n",
    "copy_to_s3(\"test_new_features.json\", s3_data_path_new_features + \"/test/test_new_features.json\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator_new_features = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo-new-features',\n",
    "    output_path=s3_output_path_new_features\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": '2H',\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"epochs\": \"400\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"num_dynamic_feat\": \"auto\",  # this will use the `dynamic_feat` field if it's present in the data\n",
    "}\n",
    "estimator_new_features.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "estimator_new_features.fit(\n",
    "    inputs={\n",
    "        \"train\": \"{}/train/\".format(s3_data_path_new_features),\n",
    "        \"test\": \"{}/test/\".format(s3_data_path_new_features)\n",
    "    }, \n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 예제와 같이, 엔드포인트를 생성하고 실시간으로 예측값을 그래프로 그려 살펴봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor_new_features = estimator_new_features.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = 120\n",
    "predictor_new_features.predict(\n",
    "    ts=time_series_processed[customer_id][:-prediction_length], \n",
    "    dynamic_feat=[special_day_features[customer_id].tolist()], \n",
    "    quantiles=[0.1, 0.5, 0.9]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 예제와 같이, 임의의 timeseries와 시점에 대하여 엔드포인트를 호출하고 예측값을 살펴봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(\n",
    "    customer_id=IntSlider(min=0, max=369, value=13, style=style), \n",
    "    forecast_day=IntSlider(min=0, max=100, value=21, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    missing_ratio=FloatSlider(min=0.0, max=0.95, value=0.2, step=0.05, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(customer_id, forecast_day, confidence, missing_ratio, show_samples): \n",
    "    forecast_date = end_training + datetime.timedelta(days=forecast_day)\n",
    "    target = time_series_processed[customer_id][start_dataset:forecast_date + prediction_length]\n",
    "    target = drop_at_random(target, missing_ratio)\n",
    "    dynamic_feat = [special_day_features[customer_id][start_dataset:forecast_date + prediction_length].tolist()]\n",
    "    plot(\n",
    "        predictor_new_features,\n",
    "        target_ts=target, \n",
    "        dynamic_feat=dynamic_feat,\n",
    "        forecast_date=forecast_date,\n",
    "        show_samples=show_samples, \n",
    "        plot_history=7*12,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_features.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
